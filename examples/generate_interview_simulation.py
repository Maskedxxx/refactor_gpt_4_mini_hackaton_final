# examples/generate_interview_simulation.py
# --- agent_meta ---
# role: interview-simulation-cli
# owner: @backend
# contract: CLI: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–º—É–ª—è—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–≤—å—é –∏–∑ ResumeInfo –∏ VacancyInfo (–æ–Ω–ª–∞–π–Ω/–æ—Ñ–ª–∞–π–Ω)
# last_reviewed: 2025-08-18
# interfaces:
#   - main(resume_json: Path, vacancy_json: Path, fake_llm: bool) -> int
# --- /agent_meta ---

from __future__ import annotations

import argparse
import asyncio
import json
import os
import uuid
from pathlib import Path
from typing import Any
import time
from datetime import datetime

from src.utils import get_logger
from src.parsing.llm.client import LLMClient
from pydantic import BaseModel
from src.parsing.resume.parser import LLMResumeParser
from src.parsing.vacancy.mapper import map_hh_json_to_vacancy
from src.models import ResumeInfo, VacancyInfo
# –í–ê–ñ–ù–û: –∏–º–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª–µ–π —Å–∏–º—É–ª—è—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ main() –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
# INTERVIEW_SIM_CONFIG (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω –ø—É—Ç—å –∫ YAML), —á—Ç–æ–±—ã –∫–æ–Ω—Ñ–∏–≥ –ø–æ–¥—Ö–≤–∞—Ç–∏–ª—Å—è –¥–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥—É–ª–µ–π.

log = get_logger("examples.generate_interview_simulation")


def _read_json(path: Path) -> dict[str, Any]:
    """–ß–∏—Ç–∞–µ—Ç JSON —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ."""
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


class TracingLLMClient(LLMClient):
    """LLM –∫–ª–∏–µ–Ω—Ç —Å –ø–æ–ª–Ω—ã–º —Ç—Ä–µ–π—Å–∏–Ω–≥–æ–º –≤—Å–µ—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤."""
    
    def __init__(self, base_llm: LLMClient, trace_file: Path):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –±–∞–∑–æ–≤—ã–º LLM –∏ —Ñ–∞–π–ª–æ–º —Ç—Ä–µ–π—Å–∏–Ω–≥–∞."""
        self.base_llm = base_llm
        self.trace_file = trace_file
        self.traces = []
        self.call_count = 0
    
    async def generate_structured(self, prompt, schema, *, model_name=None, temperature=0.0, max_tokens=None):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å —Ç—Ä–µ–π—Å–∏–Ω–≥–æ–º."""
        self.call_count += 1
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç
        prompt_text = self._extract_prompt_text(prompt)
        
        trace_entry = {
            "call_id": self.call_count,
            "timestamp": time.time(),
            "model_name": model_name or "default",
            "temperature": temperature,
            "max_tokens": max_tokens,
            "prompt": prompt_text,
            "schema": str(schema),
            "response": None,
            "error": None
        }
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = await self.base_llm.generate_structured(
                prompt, schema, 
                model_name=model_name, 
                temperature=temperature, 
                max_tokens=max_tokens
            )
            trace_entry["response"] = str(response)
            
        except Exception as e:
            trace_entry["error"] = str(e)
            raise
        finally:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç—Ä–µ–π—Å
            self.traces.append(trace_entry)
            self._save_trace()
        
        return response
    
    def _extract_prompt_text(self, prompt) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è."""
        if hasattr(prompt, 'system') and hasattr(prompt, 'user'):
            return f"SYSTEM: {prompt.system}\n\nUSER: {prompt.user}"
        elif hasattr(prompt, 'content'):
            return prompt.content
        else:
            return str(prompt)
    
    def _save_trace(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Ç—Ä–µ–π—Å—ã –≤ —Ñ–∞–π–ª."""
        trace_data = {
            "session_info": {
                "total_calls": len(self.traces),
                "created_at": time.time(),
                "trace_file": str(self.trace_file)
            },
            "traces": self.traces
        }
        
        with self.trace_file.open("w", encoding="utf-8") as f:
            json.dump(trace_data, f, ensure_ascii=False, indent=2)


class _FakeLLMForInterview(LLMClient):
    """Fake LLM –∫–ª–∏–µ–Ω—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –±–µ–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö API –≤—ã–∑–æ–≤–æ–≤."""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å —Å—á–µ—Ç—á–∏–∫–æ–º –≤—ã–∑–æ–≤–æ–≤."""
        self.call_count = 0
        self.hr_questions = [
            "–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å! –†–∞—Å—Å–∫–∞–∂–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ —Å–µ–±–µ –∏ —Å–≤–æ—ë–º –æ–ø—ã—Ç–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞ Python.",
            "–û—Ç–ª–∏—á–Ω–æ! –†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ –æ –≤–∞—à–µ–º –ø—Ä–æ–µ–∫—Ç–µ –Ω–∞ Django - –∫–∞–∫–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤—ã —Ä–µ—à–∞–ª–∏?",
            "–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ! –û–ø–∏—à–∏—Ç–µ —Å–∏—Ç—É–∞—Ü–∏—é, –∫–æ–≥–¥–∞ –≤–∞–º –ø—Ä–∏—à–ª–æ—Å—å —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –∫–æ–º–∞–Ω–¥–µ –Ω–∞–¥ —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–µ–π. –ö–∞–∫ –≤—ã –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å?",
            "–•–æ—Ä–æ—à–æ! –ß—Ç–æ –º–æ—Ç–∏–≤–∏—Ä—É–µ—Ç –≤–∞—Å –≤ —Ä–∞–±–æ—Ç–µ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞? –ü–æ—á–µ–º—É –≤—ã–±—Ä–∞–ª–∏ –∏–º–µ–Ω–Ω–æ –Ω–∞—à—É –∫–æ–º–ø–∞–Ω–∏—é?",
            "–°–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ–¥—Ä–æ–±–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã! –ï—Å—Ç—å –ª–∏ —É –≤–∞—Å –≤–æ–ø—Ä–æ—Å—ã –∫–æ –º–Ω–µ –æ –∫–æ–º–ø–∞–Ω–∏–∏ –∏–ª–∏ –ø—Ä–æ–µ–∫—Ç–∞—Ö?"
        ]
        self.candidate_answers = [
            "–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ò–≤–∞–Ω, —è Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ —Å 3-–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º. –ù–∞—á–∏–Ω–∞–ª –∫–∞–∫ Junior –≤ –Ω–µ–±–æ–ª—å—à–æ–π —Å—Ç—É–¥–∏–∏, —Å–µ–π—á–∞—Å —Ä–∞–±–æ—Ç–∞—é Middle —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–º –≤ –¢–µ—Ö–°–æ—Ñ—Ç. –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Å—å –Ω–∞ –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ —Å Django –∏ FastAPI, –µ—Å—Ç—å –æ–ø—ã—Ç —Å PostgreSQL –∏ Docker.",
            "–í –ø–æ—Å–ª–µ–¥–Ω–µ–º –ø—Ä–æ–µ–∫—Ç–µ —Å–æ–∑–¥–∞–≤–∞–ª —Å–∏—Å—Ç–µ–º—É —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–∫–∞–∑–∞–º–∏ –¥–ª—è e-commerce. –û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –±—ã–ª–∞ –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –±–∞–∑–µ - –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–∞–ª–∞—Å—å 5 —Å–µ–∫—É–Ω–¥. –î–æ–±–∞–≤–∏–ª –∏–Ω–¥–µ–∫—Å—ã, –ø–µ—Ä–µ–ø–∏—Å–∞–ª –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø—Ä–æ—Å—ã —á–µ—Ä–µ–∑ select_related, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ Redis. –í –∏—Ç–æ–≥–µ —Å–Ω–∏–∑–∏–ª –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ 0.8 —Å–µ–∫—É–Ω–¥.",
            "–ù–µ–¥–∞–≤–Ω–æ –º—ã –¥–µ–ª–∞–ª–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å –≤–Ω–µ—à–Ω–∏–º API –ø–ª–∞—Ç–µ–∂–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã. –Ø –±—ã–ª –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–º –∑–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É. –†–∞–∑–±–∏–ª –∫–æ–º–∞–Ω–¥—É: –æ–¥–∏–Ω –¥–µ–ª–∞–ª –º–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö, –¥—Ä—É–≥–æ–π - API –∫–ª–∏–µ–Ω—Ç, —è –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–ª –∏ –¥–µ–ª–∞–ª –æ—Å–Ω–æ–≤–Ω—É—é –ª–æ–≥–∏–∫—É. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ code review, daily standups. –ü—Ä–æ–µ–∫—Ç –∑–∞–≤–µ—Ä—à–∏–ª–∏ –≤ —Å—Ä–æ–∫ –±–µ–∑ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –±–∞–≥–æ–≤.",
            "–ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –∏ –≤–∏–¥–µ—Ç—å –∫–∞–∫ –º–æ–π –∫–æ–¥ –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º. –í –≤–∞—à–µ–π –∫–æ–º–ø–∞–Ω–∏–∏ –ø—Ä–∏–≤–ª–µ–∫–∞–µ—Ç —Ä–∞–±–æ—Ç–∞ —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞–∑–≤–∏–≤–∞—Ç—å—Å—è –≤ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ. –ò–∑—É—á–∞–ª –≤–∞—à–∏ –ø—Ä–æ–µ–∫—Ç—ã –Ω–∞ GitHub - –æ—á–µ–Ω—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–¥!",
            "–î–∞! –ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –≤—ã–∑–æ–≤—ã —Å–µ–π—á–∞—Å —Å—Ç–æ—è—Ç –ø–µ—Ä–µ–¥ –∫–æ–º–∞–Ω–¥–æ–π? –ò –∫–∞–∫ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω –ø—Ä–æ—Ü–µ—Å—Å code review?"
        ]
    
    async def generate_structured(self, prompt, schema, *, model_name=None, temperature=0.0, max_tokens=None):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–µ–π–∫–æ–≤—ã–µ –æ—Ç–≤–µ—Ç—ã –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏."""
        # –ò–º–∏—Ç–∏—Ä—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É API
        await asyncio.sleep(0.5)
        
        self.call_count += 1
        
        # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ (HR –∏–ª–∏ Candidate –æ—Ç–≤–µ—Ç)
        if schema == str:
            prompt_text = prompt.system.lower() if hasattr(prompt, 'system') else str(prompt).lower()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø—Ä–æ–º–ø—Ç–∞
            if "hr-–º–µ–Ω–µ–¥–∂–µ—Ä" in prompt_text or "–æ–ø—ã—Ç–Ω—ã–π hr" in prompt_text:
                # HR –≤–æ–ø—Ä–æ—Å
                question_index = min(len(self.hr_questions) - 1, (self.call_count - 1) // 2)
                return self.hr_questions[question_index]
            else:
                # Candidate –æ—Ç–≤–µ—Ç
                answer_index = min(len(self.candidate_answers) - 1, (self.call_count - 2) // 2)
                return self.candidate_answers[answer_index]
        
        # –ï—Å–ª–∏ –æ–∂–∏–¥–∞–µ—Ç—Å—è Pydantic-–º–æ–¥–µ–ª—å —Å —Ç–µ–∫—Å—Ç–æ–º, —Å–æ–±–∏—Ä–∞–µ–º –µ—ë
        try:
            if isinstance(schema, type) and issubclass(schema, BaseModel):
                prompt_text = prompt.system.lower() if hasattr(prompt, 'system') else str(prompt).lower()
                if "hr-–º–µ–Ω–µ–¥–∂–µ—Ä" in prompt_text or "–æ–ø—ã—Ç–Ω—ã–π hr" in prompt_text:
                    idx = min(len(self.hr_questions) - 1, (self.call_count - 1) // 2)
                    return schema.model_validate({"text": self.hr_questions[idx]})
                else:
                    idx = min(len(self.candidate_answers) - 1, (self.call_count - 2) // 2)
                    return schema.model_validate({"text": self.candidate_answers[idx]})
        except Exception:
            pass
        # Fallback
        return "Fake response"


async def _load_resume(resume_pdf: Path | None, fake_llm: bool) -> ResumeInfo:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—é–º–µ –∏–∑ PDF –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç fake –¥–∞–Ω–Ω—ã–µ."""
    if fake_llm or resume_pdf is None:
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fake —Ä–µ–∑—é–º–µ –Ω–∞–ø—Ä—è–º—É—é
        return ResumeInfo.model_validate({
            "first_name": "–ú–∞–∫—Å–∏–º",
            "last_name": "–ù–µ–º–æ–≤",
            "title": "NLP LLM Engineer",
            "total_experience": 48,
            "skills": "AI Engineer —Å 3+ –≥–æ–¥–∞–º–∏ —Å–æ–∑–¥–∞–Ω–∏—è production LLM-—Ä–µ—à–µ–Ω–∏–π –∏ RAG-—Å–∏—Å—Ç–µ–º. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–ª –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å—ã —Å impact $50K+ —ç–∫–æ–Ω–æ–º–∏–∏, –ø–æ–≤—ã—Å–∏–ª –∫–æ–Ω–≤–µ—Ä—Å–∏—é –ø—Ä–æ–¥–∞–∂ –Ω–∞ 35% –∏ —Ç–æ—á–Ω–æ—Å—Ç—å –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –Ω–∞ 40%. –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Å—å –Ω–∞ conversational AI, voice —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö –∏ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö AI-–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö.",
            "skill_set": ["Python", "LLM", "LangChain", "Docker", "SQL", "Git", "vLLM", "RAG Systems", "Fine-tuning", "Prompt Engineering"],
            "experience": [
                {
                    "company": "–°–æ—é–∑—Å–Ω–∞–±",
                    "position": "AI engineer",
                    "start": "2024-05",
                    "end": None,
                    "description": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–ª HR-–ø—Ä–æ—Ü–µ—Å—Å—ã, —Å–æ–∫—Ä–∞—Ç–∏–≤ –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–π —Å 3+ —á–∞—Å–æ–≤ –¥–æ 30 —Å–µ–∫—É–Ω–¥. –ü–æ–≤—ã—Å–∏–ª —Ç–æ—á–Ω–æ—Å—Ç—å –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –Ω–∞ 40% —á–µ—Ä–µ–∑ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ LLM-–∞–Ω—Å–∞–º–±–ª—è. –°–æ–∑–¥–∞–ª –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∞–Ω—Å–∞–º–±–ª—å –ª–æ–∫–∞–ª—å–Ω—ã—Ö LLM (Llama 3: 7B + 70B –º–æ–¥–µ–ª–∏)."
                },
                {
                    "company": "–ê–∫–∞–¥–µ–º–∏—è –∞—Å—Ç—Ä–æ–ª–æ–≥–∏–∏ –ò—Ä–∏–Ω—ã –ß–∞–π–∫–∏",
                    "position": "AI Engineer",
                    "start": "2023-05",
                    "end": "2024-05",
                    "description": "–£–≤–µ–ª–∏—á–∏–ª –∫–æ–Ω–≤–µ—Ä—Å–∏—é –ø—Ä–æ–¥–∞–∂ –Ω–∞ 35% —á–µ—Ä–µ–∑ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π AI-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–ª 65% —Ä–∞–±–æ—Ç—ã –∫—É—Ä–∞—Ç–æ—Ä–æ–≤, —Å—ç–∫–æ–Ω–æ–º–∏–≤ $50K+ –≥–æ–¥–æ–≤—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤. –†–∞–∑—Ä–∞–±–æ—Ç–∞–ª –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —á–∞—Ç-–±–æ—Ç–∞ –Ω–∞ –±–∞–∑–µ GPT-4."
                }
            ],
            "education": {
                "level": {"name": "–í—ã—Å—à–µ–µ"},
                "primary": [{
                    "name": "–ú–ì–£ –∏–º. –õ–æ–º–æ–Ω–æ—Å–æ–≤–∞",
                    "result": "–ú–∞–≥–∏—Å—Ç—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏",
                    "year": 2020
                }]
            },
            "languages": [
                {"name": "–†—É—Å—Å–∫–∏–π", "level": {"name": "–†–æ–¥–Ω–æ–π"}},
                {"name": "–ê–Ω–≥–ª–∏–π—Å–∫–∏–π", "level": {"name": "B2 ‚Äî –í—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ"}}
            ],
            "contact": [{"type": {"name": "–≠–ª. –ø–æ—á—Ç–∞"}, "value": "maksim.nemov@example.com"}],
            "site": [{"type": {"name": "GitHub"}, "url": "https://github.com/mnemov"}],
            "employments": ["–ü–æ–ª–Ω–∞—è –∑–∞–Ω—è—Ç–æ—Å—Ç—å"],
            "schedules": ["–ü–æ–ª–Ω—ã–π –¥–µ–Ω—å"],
            "professional_roles": [{"name": "AI Engineer"}],
            "salary": {"amount": 200000}
        })
    else:
        # –†–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ PDF
        parser = LLMResumeParser()
        return await parser.parse(resume_pdf)


async def main(resume_source: Path | None, vacancy_json: Path, fake_llm: bool = False, trace: bool = False, save_result: bool = False) -> int:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–º—É–ª—è—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–≤—å—é.
    
    Args:
        resume_json: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å —Ä–µ–∑—é–º–µ
        vacancy_json: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å –≤–∞–∫–∞–Ω—Å–∏–µ–π  
        fake_llm: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fake LLM –≤–º–µ—Å—Ç–æ —Ä–µ–∞–ª—å–Ω–æ–≥–æ OpenAI
        
    Returns:
        int: –ö–æ–¥ –≤–æ–∑–≤—Ä–∞—Ç–∞ (0 = —É—Å–ø–µ—Ö)
    """
    log.info("üé≠ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–º—É–ª—è—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–≤—å—é")
    
    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        log.info("üìñ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ä–µ–∑—é–º–µ –∏ –≤–∞–∫–∞–Ω—Å–∏–∏")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ä–µ–∑—é–º–µ (PDF –∏–ª–∏ fake)
        if resume_source and resume_source.suffix.lower() == '.pdf':
            log.info(f"üìÑ –ü–∞—Ä—Å–∏–º PDF —Ä–µ–∑—é–º–µ: {resume_source}")
            resume = await _load_resume(resume_source, fake_llm)
        else:
            log.info("üé≠ –ò—Å–ø–æ–ª—å–∑—É–µ–º fake —Ä–µ–∑—é–º–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏")
            resume = await _load_resume(None, True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏—é
        vacancy_data = _read_json(vacancy_json)
        if 'id' in vacancy_data:  # HH.ru —Ñ–æ—Ä–º–∞—Ç
            vacancy = map_hh_json_to_vacancy(vacancy_data)
        else:
            vacancy = VacancyInfo.model_validate(vacancy_data)
        
        log.info(f"   –ö–∞–Ω–¥–∏–¥–∞—Ç: {resume.first_name} {resume.last_name}")
        log.info(f"   –ü–æ–∑–∏—Ü–∏—è: {vacancy.name}")
        
        # 2. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å —Ç—Ä–µ–π—Å–∏–Ω–≥–æ–º
        generator_kwargs = {}
        trace_file = None
        
        if trace:
            # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞
            trace_dir = Path("output/traces")
            trace_dir.mkdir(parents=True, exist_ok=True)
            trace_file = trace_dir / f"interview_trace_{uuid.uuid4().hex[:8]}.json"
            log.info(f"üìä –í–∫–ª—é—á–µ–Ω –ø–æ–ª–Ω—ã–π —Ç—Ä–µ–π—Å–∏–Ω–≥: {trace_file}")
        
        if fake_llm:
            log.info("üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º fake LLM –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏")
            base_llm = _FakeLLMForInterview()
        else:
            log.info("üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π OpenAI API")
            if not os.getenv("OPENAI_API_KEY"):
                log.error("‚ùå OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --fake-llm –¥–ª—è –¥–µ–º–æ")
                return 1
            # –ö–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ–º OpenAI SDK –∫–ª–∏–µ–Ω—Ç –∏ –ø–µ—Ä–µ–¥–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—É—é –º–æ–¥–µ–ª—å –≤ –∞–¥–∞–ø—Ç–µ—Ä
            from openai import OpenAI
            from src.parsing.llm.client import OpenAILLMClient
            # –ú–æ–¥–µ–ª—å –±–µ—Ä–µ–º –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∏–∑ YAML/–¥–µ—Ñ–æ–ª—Ç–æ–≤ –º–æ–¥—É–ª—è
            try:
                from src.llm_interview_simulation.config import default_settings as _sim_defaults
                default_model = os.getenv("OPENAI_MODEL_NAME", _sim_defaults.openai_model_name)
            except Exception:
                default_model = os.getenv("OPENAI_MODEL_NAME", "gpt-4o-mini-2024-07-18")
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            base_llm = OpenAILLMClient(client, default_model)
        
        # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ —Ç—Ä–µ–π—Å–∏–Ω–≥ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if trace and trace_file:
            generator_kwargs["llm"] = TracingLLMClient(base_llm, trace_file)
        else:
            generator_kwargs["llm"] = base_llm
        
        # –ò–º–ø–æ—Ä—Ç—ã –ø–æ—Å–ª–µ –≤–æ–∑–º–æ–∂–Ω–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏ INTERVIEW_SIM_CONFIG
        from src.llm_interview_simulation import (
            LLMInterviewSimulationGenerator,
            InterviewSimulationOptions,
        )

        generator = LLMInterviewSimulationGenerator(**generator_kwargs)
        
        # 3. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–ø—Ü–∏–∏
        options = InterviewSimulationOptions(
            prompt_version="v1.0",
            target_rounds=5,
            difficulty_level="medium",
            include_behavioral=True,
            include_technical=True,
            hr_personality="neutral",
            candidate_confidence="medium",
            temperature_hr=0.7,
            temperature_candidate=0.8,
            log_detailed_prompts=True  # –í–∫–ª—é—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤
        )
        
        log.info(f"üéØ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {options.target_rounds} —Ä–∞—É–Ω–¥–æ–≤, —Å–ª–æ–∂–Ω–æ—Å—Ç—å {options.difficulty_level}")
        
        # 4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–º—É–ª—è—Ü–∏—é –∏–Ω—Ç–µ—Ä–≤—å—é
        log.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Å–∏–º—É–ª—è—Ü–∏—é –∏–Ω—Ç–µ—Ä–≤—å—é...")
        start_time = time.time()
        
        result = await generator.generate(resume, vacancy, options)
        
        elapsed_time = time.time() - start_time
        log.info(f"‚úÖ –°–∏–º—É–ª—è—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.1f} —Å–µ–∫—É–Ω–¥")
        
        # 5. –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print("\n" + "=" * 80)
        print("üé≠ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–ò–ú–£–õ–Ø–¶–ò–ò –ò–ù–¢–ï–†–í–¨–Æ")
        print("=" * 80)
        
        print("\nüìã –ë–ê–ó–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:")
        print(f"   –ü–æ–∑–∏—Ü–∏—è: {result.position_title}")
        print(f"   –ö–∞–Ω–¥–∏–¥–∞—Ç: {result.candidate_name}")
        print(f"   –£—Ä–æ–≤–µ–Ω—å: {result.candidate_profile.detected_level.value}")
        print(f"   –†–æ–ª—å: {result.candidate_profile.detected_role.value}")
        print(f"   –û–ø—ã—Ç: {result.candidate_profile.years_of_experience} –ª–µ—Ç")
        
        print("\nüéØ –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ù–¢–ï–†–í–¨–Æ:")
        print(f"   –†–∞—É–Ω–¥–æ–≤ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ: {result.total_rounds_completed}")
        print(f"   –¢–∏–ø—ã –≤–æ–ø—Ä–æ—Å–æ–≤: {', '.join([qt.value for qt in result.covered_question_types])}")
        
        print("\nüí¨ –î–ò–ê–õ–û–ì –ò–ù–¢–ï–†–í–¨–Æ:")
        for i, msg in enumerate(result.dialog_messages, 1):
            speaker_icon = "üë§ HR" if msg.speaker == "HR" else "ü§µ –ö–∞–Ω–¥–∏–¥–∞—Ç"
            quality_info = ""
            question_type_info = f" [{msg.question_type.value}]" if msg.question_type else ""
            
            print(f"\n   {i}. {speaker_icon} (—Ä–∞—É–Ω–¥ {msg.round_number}){question_type_info}{quality_info}:")
            print(f"      {msg.message}")
        
        # –ë–ª–æ–∫ –æ—Ü–µ–Ω–∫–∏ —É–¥–∞–ª–µ–Ω –≤ —ç—Ç–æ–π –≤–µ—Ä—Å–∏–∏
        
        # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è PDF —ç–∫—Å–ø–æ—Ä—Ç–∞ (–µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ)
        if save_result:
            tests_data_dir = Path("tests/data")
            tests_data_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            test_result_file = tests_data_dir / f"interview_simulation_result_{timestamp}.json"
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ JSON-—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç
            result_dict = result.model_dump() if hasattr(result, 'model_dump') else result.dict()
            
            with test_result_file.open("w", encoding="utf-8") as f:
                json.dump(result_dict, f, ensure_ascii=False, indent=2)
            
            print("\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –¥–ª—è PDF —ç–∫—Å–ø–æ—Ä—Ç–∞:")
            print(f"   üìÅ {test_result_file}")
            print(f"   üì¶ –î–ª—è PDF –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: python examples/test_pdf_export.py --feature interview_simulation --result-file {test_result_file.name}")
            print("=" * 80)
            return 0
        
        # –û–±—ã—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ output/
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)
        
        session_id = uuid.uuid4().hex[:8]
        result_file = output_dir / f"interview_simulation_{session_id}.json"
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ JSON-—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç
        result_dict = result.model_dump() if hasattr(result, 'model_dump') else result.dict()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        with result_file.open("w", encoding="utf-8") as f:
            json.dump(result_dict, f, ensure_ascii=False, indent=2)
        
        log.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {result_file}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω (–≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ + —Ä–µ–∑—É–ª—å—Ç–∞—Ç + —Ç—Ä–µ–π—Å)
        if trace and trace_file:
            pipeline_file = output_dir / f"full_pipeline_{session_id}.json"
            
            pipeline_data = {
                "pipeline_info": {
                    "session_id": session_id,
                    "timestamp": time.time(),
                    "resume_source": str(resume_source) if resume_source else "fake_data",
                    "vacancy_source": str(vacancy_json),
                    "fake_llm": fake_llm,
                    "trace_enabled": trace
                },
                "input_data": {
                    "resume": result_dict.get("resume_info", {}),
                    "vacancy": result_dict.get("vacancy_info", {})
                },
                "simulation_result": result_dict,
                "trace_file": str(trace_file)
            }
            
            with pipeline_file.open("w", encoding="utf-8") as f:
                json.dump(pipeline_data, f, ensure_ascii=False, indent=2)
            
            print("\nüìä –ü–û–õ–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù –°–û–•–†–ê–ù–ï–ù:")
            print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {result_file}")
            print(f"   –¢—Ä–µ–π—Å –ø—Ä–æ–º–ø—Ç–æ–≤: {trace_file}")
            print(f"   –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: {pipeline_file}")
        else:
            print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {result_file}")
            if not trace:
                print("   üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --trace –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –ø—Ä–æ–º–ø—Ç–æ–≤")
        
        print("=" * 80)
        
        return 0
        
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–º—É–ª—è—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return 1


def create_sample_data():
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    sample_resume = {
        "first_name": "–ò–≤–∞–Ω",
        "last_name": "–ü–µ—Ç—Ä–æ–≤", 
        "title": "Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫",
        "skills": "–û–ø—ã—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –Ω–∞ Python —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Django –∏ FastAPI. –ó–Ω–∞–Ω–∏–µ SQL, Git, Docker. –£—á–∞—Å—Ç–∏–µ –≤ –ø—Ä–æ–µ–∫—Ç–∞—Ö –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–æ –¥–µ–ø–ª–æ—è.",
        "skill_set": ["Python", "Django", "FastAPI", "PostgreSQL", "Git", "Docker", "Redis", "REST API"],
        "experience": [
            {
                "company": "–û–û–û –¢–µ—Ö–°–æ—Ñ—Ç",
                "position": "Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫",
                "start": "2022-01",
                "end": "2024-12",
                "description": "–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –Ω–∞ Django. –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ö–æ–¥-—Ä–µ–≤—å—é. –ù–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ Junior —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤."
            },
            {
                "company": "–ò–ü –°–∏–¥–æ—Ä–æ–≤",
                "position": "Junior Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫", 
                "start": "2021-06",
                "end": "2021-12",
                "description": "–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ REST API –Ω–∞ FastAPI. –†–∞–±–æ—Ç–∞ —Å PostgreSQL. –ù–∞–ø–∏—Å–∞–Ω–∏–µ unit-—Ç–µ—Å—Ç–æ–≤. –ò–∑—É—á–µ–Ω–∏–µ Docker –∏ DevOps –ø—Ä–∞–∫—Ç–∏–∫."
            }
        ],
        "education": {
            "level": {"name": "–í—ã—Å—à–µ–µ"},
            "primary": [
                {
                    "name": "–ú–ì–£ –∏–º. –õ–æ–º–æ–Ω–æ—Å–æ–≤–∞, –§–∞–∫—É–ª—å—Ç–µ—Ç –í–ú–ö",
                    "result": "–ë–∞–∫–∞–ª–∞–≤—Ä –ø—Ä–∏–∫–ª–∞–¥–Ω–æ–π –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏",
                    "year": "2021"
                }
            ]
        },
        "total_experience": 36,
        "salary": {"amount": 150000},
        "professional_roles": [
            {"name": "Backend —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫"},
            {"name": "Python Developer"}
        ],
        "languages": [
            {"name": "–†—É—Å—Å–∫–∏–π", "level": {"name": "–†–æ–¥–Ω–æ–π"}},
            {"name": "–ê–Ω–≥–ª–∏–π—Å–∫–∏–π", "level": {"name": "B2 ‚Äî –í—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ"}}
        ],
        "contact": [
            {"type": {"name": "–≠–ª. –ø–æ—á—Ç–∞"}, "value": "ivan.petrov@example.com"}
        ],
        "site": [
            {"type": {"name": "GitHub"}, "url": "https://github.com/apetrov"}
        ],
        "employments": ["–ü–æ–ª–Ω–∞—è –∑–∞–Ω—è—Ç–æ—Å—Ç—å"],
        "schedules": ["–ü–æ–ª–Ω—ã–π –¥–µ–Ω—å"]
    }
    
    sample_vacancy = {
        "name": "Middle Python Developer",
        "company_name": "IT Solutions Ltd",
        "description": """
        <p>–ú—ã –∏—â–µ–º –æ–ø—ã—Ç–Ω–æ–≥–æ Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã –Ω–∞–¥ –∫—Ä—É–ø–Ω—ã–º–∏ –ø—Ä–æ–µ–∫—Ç–∞–º–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ —Ñ–∏–Ω—Ç–µ—Ö.</p>
        <h3>–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:</h3>
        <ul>
            <li>–û–ø—ã—Ç –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞ Python –æ—Ç 2-—Ö –ª–µ—Ç</li>
            <li>–ì–ª—É–±–æ–∫–æ–µ –∑–Ω–∞–Ω–∏–µ Django –∏–ª–∏ FastAPI</li>
            <li>–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Å PostgreSQL, –∑–Ω–∞–Ω–∏–µ SQL</li>
            <li>–ü–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ REST API</li>
            <li>–ó–Ω–∞–Ω–∏–µ Git, –ø–æ–Ω–∏–º–∞–Ω–∏–µ Git Flow</li>
            <li>–û–ø—ã—Ç –Ω–∞–ø–∏—Å–∞–Ω–∏—è unit-—Ç–µ—Å—Ç–æ–≤</li>
        </ul>
        <h3>–ë—É–¥–µ—Ç –ø–ª—é—Å–æ–º:</h3>
        <ul>
            <li>–û–ø—ã—Ç —Å Docker –∏ Kubernetes</li>
            <li>–ó–Ω–∞–Ω–∏–µ Redis</li>
            <li>–û–ø—ã—Ç —Å –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π</li>
            <li>–ó–Ω–∞–Ω–∏–µ JavaScript/TypeScript</li>
        </ul>
        <h3>–ú—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º:</h3>
        <ul>
            <li>–†–∞–±–æ—Ç—É –≤ –∫–æ–º–∞–Ω–¥–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–æ–≤</li>
            <li>–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –∏ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏</li>
            <li>–ì–∏–±–∫–∏–π –≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã</li>
            <li>–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞</li>
        </ul>
        """,
        "key_skills": ["Python", "Django", "FastAPI", "PostgreSQL", "REST API", "Git", "Docker"],
        "professional_roles": [{"name": "Backend —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫"}],
        "experience": {"id": "between1And3"},
        "employment": {"id": "full"},
        "schedule": {"id": "fullDay"}
    }
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    data_dir = Path("tests/data")
    data_dir.mkdir(exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    resume_file = data_dir / "sample_interview_resume.json"
    vacancy_file = data_dir / "sample_interview_vacancy.json"
    
    with resume_file.open("w", encoding="utf-8") as f:
        json.dump(sample_resume, f, ensure_ascii=False, indent=2)
    
    with vacancy_file.open("w", encoding="utf-8") as f:
        json.dump(sample_vacancy, f, ensure_ascii=False, indent=2)
    
    print("üìù –ü—Ä–∏–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã:")
    print(f"   –†–µ–∑—é–º–µ: {resume_file}")
    print(f"   –í–∞–∫–∞–Ω—Å–∏—è: {vacancy_file}")
    
    return resume_file, vacancy_file


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–º—É–ª—è—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–≤—å—é —Å –ø–æ–ª–Ω—ã–º —Ç—Ä–µ–π—Å–∏–Ω–≥–æ–º")
    parser.add_argument(
        "--resume-pdf", 
        type=Path, 
        help="–ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É —Ä–µ–∑—é–º–µ"
    )
    parser.add_argument(
        "--vacancy-json", 
        type=Path, 
        help="–ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å –≤–∞–∫–∞–Ω—Å–∏–µ–π"
    )
    parser.add_argument(
        "--fake-llm", 
        action="store_true", 
        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fake LLM –≤–º–µ—Å—Ç–æ OpenAI (–¥–ª—è –¥–µ–º–æ)"
    )
    parser.add_argument(
        "--trace", 
        action="store_true", 
        help="–í–∫–ª—é—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç—Ä–µ–π—Å–∏–Ω–≥ –≤—Å–µ—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤"
    )
    parser.add_argument(
        "--create-sample", 
        action="store_true", 
        help="–°–æ–∑–¥–∞—Ç—å –ø—Ä–∏–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"
    )
    parser.add_argument(
        "--config",
        type=Path,
        help="–ü—É—Ç—å –∫ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π config.yml)"
    )
    parser.add_argument(
        "--save-result",
        action="store_true",
        help="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ tests/data/ –¥–ª—è PDF —ç–∫—Å–ø–æ—Ä—Ç–∞"
    )
    
    args = parser.parse_args()
    
    if args.create_sample:
        _, vacancy_file = create_sample_data()
        args.vacancy_json = vacancy_file
        args.resume_pdf = None  # –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω fake —Ä–µ–∂–∏–º
        print("üìù –°–æ–∑–¥–∞–Ω—ã —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω fake —Ä–µ–∂–∏–º –¥–ª—è —Ä–µ–∑—é–º–µ.")
    
    if not args.vacancy_json:
        print("‚ùå –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --create-sample")
        print("–ü—Ä–∏–º–µ—Ä—ã:")
        print("  python examples/generate_interview_simulation.py --create-sample --fake-llm --trace")
        print("  python examples/generate_interview_simulation.py --resume-pdf resume.pdf --vacancy-json vacancy.json --trace")
        exit(1)
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É –¥–æ –∏–º–ø–æ—Ä—Ç–æ–≤ –º–æ–¥—É–ª–µ–π —Å–∏–º—É–ª—è—Ü–∏–∏
    if args.config:
        os.environ["INTERVIEW_SIM_CONFIG"] = str(args.config.expanduser())

    exit_code = asyncio.run(main(args.resume_pdf, args.vacancy_json, args.fake_llm, args.trace, getattr(args, 'save_result', False)))
    exit(exit_code)
